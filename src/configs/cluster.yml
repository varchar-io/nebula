version: 1.0

# server configs
server:
  # as node will treat server to run as a node
  # if false, server will not load any data in itself.
  # NOTE that, there are maybe missing functions to fully compatible for server as node.
  # such as task execution may not be implemented.
  anode: false

  # this field indicates if current deployment requires auth or not
  # if set as true, auth (such as oAuth) is required to set up to make query executable
  auth: false

  # configure meta db where metadata is persist
  # currently, we support native DB (backed by LevelDB).
  # other DB types could be extended in the future such as SQL DB or KV store.
  # store is the location where the DB data is persited.
  meta:
    db: native
    store: s3://nebula/meta

  # configure the way how service discovery is done with this cluster.
  # by default - we're able to read the hard-coded `nodes` from config file
  # other options: server itself is discovery server exposing service API.
  # also if a standalone discovery service out of nebula server.
  # method: config, service, etc.
  discovery:
    method: service

# will be provided by environment
nodes:
  - node:
      host: localhost
      port: 9199

tables:
  nebula.test:
    retention:
      # max 10G RAM assigment
      max-mb: 10000
      # max 10 days assignment
      max-hr: 240
    schema: "ROW<id:int, event:string, tag:string, items:list<string>, flag:bool, value:tinyint>"
    data: custom
    loader: NebulaTest
    source: ""
    backup: s3://nebula/n100/
    format: none
    # NOTE: refernece only, column properties defined here will not take effect
    # because they are overwritten/decided by definition of TestTable.h
    columns:
      id:
        bloom_filter: true
      event:
        access:
          read:
            groups: ["nebula-users"]
            action: mask
      tag:
        partition:
          values: ["a", "b", "c"]
          chunk: 1
    time:
      type: static
      # get it from linux by "date +%s"
      value: 1565994194
  # user.features:
  #   retention:
  #     max-mb: 10000
  #     max-hr: 0
  #   # TODO(cao): failing to ingest list type column from parquet - need investigate and fix
  #   # categories:list<bigint>, interests:list<bigint>
  #   schema: "ROW<id:bigint, country:string, metro:int, gender:tinyint, age:smallint>"
  #   data: s3
  #   loader: Swap
  #   source: <s3 location>
  #   backup: <s3 location>
  #   format: parquet
  #   columns:
  #     id:
  #       bloom_filter: true
  #     country:
  #       dict: true
  #   time:
  #     type: static
  #     value: 1595203200
  # one example message looks like:
  # {"service":"test", "host":"dev-shawncao", "tag":"dev", "lang":"java", "stack":"a\nb\nc"}
  # k.pinterest-code:
  #   retention:
  #     max-mb: 200000
  #     max-hr: 48
  #   schema: "ROW<service:string, host:string, tag:string, lang:string, stack:string>"
  #   data: kafka
  #   topic: <topic>
  #   loader: Roll
  #   source: <brokers>
  #   backup: s3://nebula/n116/
  #   format: json
  #   columns:
  #     service:
  #       dict: true
  #     host:
  #       dict: true
  #     tag:
  #       dict: true
  #     lang:
  #       dict: true
  #   time:
  #     # kafka will inject a time column when specified provided
  #     type: provided
  #   settings:
  #     batch: 500
  # # basic S3 file in csv format
  # seattle.calls:
  #   retention:
  #     max-mb: 40000
  #     max-hr: 0
  #   schema: "ROW<cad:long, clearence:string, type:string, priority:int, init_type:string, final_type:string, queue_time:string, arrive_time:string, precinct:string, sector:string, beat:string>"
  #   data: s3
  #   loader: Swap
  #   source: s3://nebula/seattle_calls.10k.tsv
  #   backup: s3://nebula/n202/
  #   format: csv
  #   time:
  #     type: column
  #     column: queue_time
  #     pattern: "%m/%d/%Y %H:%M:%S"
  #   settings:
  #     # csv delimiter - it defaults to tab key, so have to specify
  #     # try to use tab rather than comma - nebula CSV reader may have bug if column value contains comma
  #     # csv.delimiter: ","
  #     # the data has header - it defaults to true hence can be ommited.
  #     csv.header: true
  # # basic example to consume parquet file from S3
  # <table name>:
  #   retention:
  #     max-mb: 10000
  #     max-hr: 0
  #   schema: "ROW<annotation:string, language:string>"
  #   data: s3
  #   loader: Swap
  #   source: s3://bucket/path
  #   backup: s3://nebula/n107/
  #   format: parquet
  #   columns:
  #     language:
  #       dict: true
  #   time:
  #     type: static
  #     # date +%s --date='2019-09-10'
  #     value: 1571875200
  # # basic kafka stream in thrift format
  # # all kafka table name will start with "k." followed by topic name
  # # to indicating its a kafka data source
  # k.<topic>:
  #   retention:
  #     max-mb: 200000
  #     max-hr: 12
  #   # all kafka schema will have column name postfixed by field ID
  #   # TODO(cao): in the future, we can have separate thrift schema
  #   schema: "ROW<userId:long, magicType:short, statusCode:byte, objectCount:int>"
  #   data: kafka
  #   topic: <topic>
  #   loader: Roll
  #   source: <brokers>
  #   backup: s3://nebula/n105/
  #   format: thrift
  #   serde:
  #     topic-retention: 90000
  #     protocol: binary
  #     cmap:
  #       # TODO(cao): this temporary hack to work around nested thrift definition
  #       # we're using 1K to separate two levels asssuming no thrift definition has more than 1K fields
  #       # in reverse order, such as 1003 => (field 3 -> field 1)
  #       _time_: 1
  #       userId: 3001
  #       magicType: 3003
  #       statusCode: 4002
  #       objectCount: 4001
  #   columns:
  #     userId:
  #       bloom_filter: true
  #     statusCode:
  #       default_value: 0
  #     objectCount:
  #       default_value: 0
  #   time:
  #     type: current
  # # basic kafka stream in json format
  # # another kafka real time example in json format
  # k.reporting:
  #   retention:
  #     max-mb: 200000
  #     max-hr: 1
  #   schema: "ROW<time:long, ct:int, actor_id:string, pid:long, app:byte, eventtype:string, owner:long, root_id:long, crated:long, version:short, slot:int>"
  #   data: kafka
  #   topic: <topic>
  #   loader: Roll
  #   source: <brokers>
  #   backup: s3://nebula/n115/
  #   format: json
  #   columns:
  #     pid:
  #       bloom_filter: true
  #     owner:
  #       bloom_filter: true
  #     root_id:
  #       bloom_filter: true
  #     version:
  #       bloom_filter: true
  #     eventtype:
  #       dict: true
  #   time:
  #     type: column
  #     column: time
  #     pattern: UNIXTIME_NANO
  #   settings:
  #     batch: 1000000
  #     # k.partition: 11
  #     # k.offset: 3986000000
  #     # "kafka." is preserved prefix for kafka configs
  #     # kafka.queued.min.messages: 50000
  #     # kafka.queued.max.messages.kbytes: 102400
  #     # kafka.fetch.message.max.bytes: 10240
