version: 1.0

# server configs
server:
  # as node will treat server to run as a node
  # if false, server will not load any data in itself.
  # NOTE that, there are maybe missing functions to fully compatible for server as node.
  # such as task execution may not be implemented.
  anode: false

  # this field indicates if current deployment requires auth or not
  # if set as true, auth (such as oAuth) is required to set up to make query executable
  auth: false

# will be provided by enviroment
nodes:
  - node:
      host: <hostname | ip>
      port: 9199

tables:
  nebula.test:
    # max 10G RAM assigment
    max-mb: 10000
    # max 10 days assignment
    max-hr: 240
    schema: "ROW<id:int, event:string, items:list<string>, flag:bool, value:tinyint>"
    data: custom
    loader: NebulaTest
    source: ""
    backup: s3://nebula/n100/
    format: none
    # NOTE: refernece only, column properties defined here will not take effect
    # because they are overwritten/decided by definition of TestTable.h
    columns:
      id:
        bloom_filter: true
      event:
        access:
          read:
            groups: ["nebula-users"]
            action: mask
    time:
      type: static
      # get it from linux by "date +%s"
      value: 1565994194
  # <table name>:
  #   max-mb: 10000
  #   max-hr: 0
  #   schema: "ROW<annotation:string, language:string>"
  #   data: s3
  #   loader: Swap
  #   source: s3://bucket/path
  #   backup: s3://nebula/n107/
  #   format: parquet
  #   columns:
  #     language:
  #       dict: true
  #   time:
  #     type: static
  #     # date +%s --date='2019-09-10'
  #     value: 1571875200
  ## all kafka table name will start with "k." followed by topic name
  ## to indicating its a kafka data source
  # k.<topic>:
  #   max-mb: 200000
  #   max-hr: 12
  #   # all kafka schema will have column name postfixed by field ID
  #   # TODO(cao): in the future, we can have separate thrift schema
  #   schema: "ROW<userId:long, magicType:short, statusCode:byte, objectCount:int>"
  #   data: kafka
  #   topic: <topic>
  #   loader: Roll
  #   source: <brokers>
  #   backup: s3://nebula/n105/
  #   format: thrift
  #   serde:
  #     retention: 90000
  #     protocol: binary
  #     cmap:
  #       # TODO(cao): this temporary hack to work around nested thrift definition
  #       # we're using 1K to separate two levels asssuming no thrift definition has more than 1K fields
  #       # in reverse order, such as 1003 => (field 3 -> field 1)
  #       _time_: 1
  #       userId: 3001
  #       magicType: 3003
  #       statusCode: 4002
  #       objectCount: 4001
  #   columns:
  #     userId:
  #       bloom_filter: true
  #     statusCode:
  #       default_value: 0
  #     objectCount:
  #       default_value: 0
  #   time:
  #     type: current
