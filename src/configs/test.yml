version: 1.0

# server configs
server:
  # as node will treat server to run as a node
  # if false, server will not load any data in itself.
  # NOTE that, there are maybe missing functions to fully compatible for server as node.
  # such as task execution may not be implemented.
  anode: false
  auth: false

  # db could be "none" to not store anything
  meta:
    db: none

  discovery:
    method: config

# will be provided by environment
nodes:
  - node:
      host: localhost
      port: 9199

tables:
  nebula.test:
    retention:
      max-mb: 10000
      max-hr: 240
    schema: "ROW<id:int, event:string, items:list<string>, flag:bool, value:tinyint>"
    data: local
    loader: NebulaTest
    source: ""
    backup: s3://nebula/n100/
    format: none
    headers: ["header1: 1", "header2: 2"]
    settings:
      key1: value1
      key2: value2
    time:
      type: static
      # get it from linux by "date +%s"
      value: 1565994194

  nebula.daily:
    # size and time may not be respected - ephemeral
    retention:
      max-mb: 10000
      max-hr: 1
    schema: "ROW<actor_id:string, pin_id:bigint, partner_id:bigint, ct:bigint, app:string, domain:string, url:string, image_signature:string, root_pin_id:bigint, is_video:boolean, board_id:bigint, publish_type:int, create_date_ts:bigint, country:string, metro:int, gender:tinyint, age:smallint, custom_tag:int>"
    data: s3
    loader: Api
    # parameterized data source for on-demand loading, all parameters are formed by {}
    # this example template has these parameters {date}, {ds}, {ct}, {pf}, {et}, {bucket}
    source: s3://nebula/ephemeral/dt=%7Bdate%7D/downstream=%7Bds%7D/contenttype=%7Bct%7D/pinformat=%7Bpf%7D/eventtype=%7Bet%7D/part-r-%7Bbucket%7D-fb7ea820-76a3-4c60-be79-956727df7593.gz.parquet
    # this is a bucket table, so it will have a bucket macro to be fullfiled
    # this table bucketed by partner_id into 10K buckets
    bucket:
      size: 10000
      column: partner_id
    backup: s3://nebula/n200/
    format: parquet
    columns:
      app:
        dict: true
      country:
        dict: true
    time:
      type: macro
      pattern: daily

  nebula.hourly:
    # size and time may not be respected - ephemeral
    retention:
      max-mb: 10000
      max-hr: 4
    schema: "ROW<actor_id:string, pin_id:bigint, partner_id:bigint, ct:bigint, app:string, domain:string, url:string, image_signature:string, root_pin_id:bigint, is_video:boolean, board_id:bigint, publish_type:int, create_date_ts:bigint, country:string, metro:int, gender:tinyint, age:smallint, custom_tag:int>"
    data: s3
    loader: Api
    # parameterized data source for on-demand loading, all parameters are formed by {}
    # this example template has these parameters {date}, {ds}, {ct}, {pf}, {et}, {bucket}
    source: s3://nebula/ephemeral/dt=%7Bdate%7D/hr=%7Bhour%7D/downstream=%7Bds%7D/contenttype=%7Bct%7D/pinformat=%7Bpf%7D/eventtype=%7Bet%7D/part-r-%7Bbucket%7D-fb7ea820-76a3-4c60-be79-956727df7593.gz.parquet
    # this is a bucket table, so it will have a bucket macro to be fullfiled
    # this table bucketed by partner_id into 10K buckets
    bucket:
      size: 10000
      column: partner_id
    backup: s3://nebula/n200/
    format: parquet
    columns:
      app:
        dict: true
      country:
        dict: true
    time:
      type: macro
      # support granularity between DAILY, HOURLY, MINUTELY, SECONDLY
      pattern: HOURLY

  # rockset.test:
  #   retention:
  #     max-mb: 10000
  #     max-hr: 48
  #   schema: "ROW<_event_time:string, original_language:string, overview:string, vote_average:real, vote_count:integer, revenue:bigint, runtime:short, popularity:real, release_date:string, title:string>"
  #   data: rockset
  #   loader: Streaming
  #   source: https://api.rs2.usw2.rockset.com/v1/orgs/self/ws/commons/lambdas/selectall/tags/latest
  #   backup: s3://nebula/n100/
  #   format: json
  #   rockset:
  #     apiKey: <API_KEY>
  #     interval: 15
  #   json:
  #     rowsField: results
  #   time:
  #     type: column
  #     column: _event_time
  #     pattern: "%FT%T"

  # azure.datalake:
  #   retention:
  #     max-mb: 10000
  #     max-hr: 48
  #   schema: "ROW<company_id:string, timestamp:bigint, timeuuid:string, event:string, user_id:string, value:string, metadata:string>"
  #   data: abfs
  #   loader: Swap
  #   source: abfs://testevents/raw_events
  #   backup: s3://nebula/fake
  #   format: csv
  #   csv:
  #     hasHeader: false
  #     delimiter: ","
  #   columns:
  #     company_id:
  #       bloom_filter: true
  #     event:
  #       dict: true
  #     user_id:
  #       bloom_filter: true
  #   time:
  #     type: column
  #     column: timestamp
  #     pattern: UNIXTIME_MS
  #   settings:
  #     azure.storage.url: https://testnebula.dfs.core.windows.net
  #     azure.storage.account: testnebula
  #     azure.storage.secret: <insert_secret_here>

  # An exmaple of using macro in source template and extract them into partition columns
  # expect we could test this out after the work is done, and document this example afterwards.
  # macro.test:
  #   retention:
  #     max-mb: 10000
  #     max-hr: 48
  #   schema: "ROW<part:string, name:string>"
  #   data: local
  #   loader: Roll
  #   source: "file:///tmp/%7Bpart%7D/test.csv"
  #   backup: s3://nebula/n100/
  #   format: csv
  #   csv:
  #     hasHeader: true
  #     delimiter: ","
  #   macros:
  #     part: ["macro-part1", "macro-part2"]
  #   columns:
  #     part:
  #       partition:
  #         values: ["macro-part1", "macro-part2"]
  #         chunk: 1
  #       macro: part
  #   time:
  #     type: current
  oom.test:
    retention:
      max-mb: 10000
      max-hr: 48
    schema: "ROW<user_id:string, value: string,  event: string, metadata: string>"
    data: local
    loader: Swap
    source: "file:///tmp/oom.snappy.parquet"
    backup: s3://nebula/n100/
    format: parquet
    time:
      type: current
