/*
 * Copyright 2017-present varchar.io
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#pragma once

#include <fstream>
#include <iostream>

#include "SchemaHelper.h"
#include "common/Compression.h"
#include "common/Conv.h"
#include "common/Errors.h"
#include "common/Format.h"
#include "common/Hash.h"
#include "meta/TableSpec.h"
#include "storage/NFS.h"
#include "surface/DataSurface.h"

/**
 * A CSV file reader, with or without header for schema
 */
namespace nebula {
namespace storage {

// used for skipping unused rows (head + meta)
class DevNull {};
std::istream& operator>>(std::istream&, DevNull&);

class CsvRow : public nebula::surface::RowData {
public:
  CsvRow(char delimiter) : delimiter_{ delimiter } {}
  virtual ~CsvRow() = default;

  bool isNull(const std::string&) const override {
    // TODO(cao) - CSV reader doesn't produce null valus for now
    return false;
  }

#define CONV_TYPE_INDEX(TYPE, FUNC)                      \
  TYPE FUNC(const std::string& field) const override {   \
    auto index = columnLookup_(field);                   \
    auto& v = const_cast<std::string&>(data_.at(index)); \
    nebula::common::unformat<TYPE>(v);                   \
    return nebula::common::safe_to<TYPE>(v);             \
  }

  CONV_TYPE_INDEX(bool, readBool)
  CONV_TYPE_INDEX(int8_t, readByte)
  CONV_TYPE_INDEX(int16_t, readShort)
  CONV_TYPE_INDEX(int32_t, readInt)
  CONV_TYPE_INDEX(int64_t, readLong)
  CONV_TYPE_INDEX(float, readFloat)
  CONV_TYPE_INDEX(double, readDouble)
  CONV_TYPE_INDEX(int128_t, readInt128)

  std::string_view readString(const std::string& field) const override {
    auto index = columnLookup_(field);
    return data_.at(index);
  }

#undef CONV_TYPE_INDEX

  // compound types
  std::unique_ptr<nebula::surface::ListData> readList(const std::string&) const override {
    throw NException("Array not supported yet.");
  }

  std::unique_ptr<nebula::surface::MapData> readMap(const std::string&) const override {
    throw NException("Map not supported yet.");
  }

  void setData(const std::vector<std::string> data) {
    data_ = std::move(data);
  }

  void setSchema(const std::function<size_t(const std::string&)>& columnLookup) {
    columnLookup_ = columnLookup;
  }

public:
  // true if read a valid row, otherwise false
  bool readNext(std::istream&, const size_t);
  inline const std::vector<std::string>& rawData() const {
    return data_;
  }

private:
  char delimiter_;
  // reference a line generated by reader
  std::vector<std::string> data_;
  std::function<size_t(std::string)> columnLookup_;
};

class CsvReader : public nebula::surface::RowCursor {
public:
  CsvReader(const std::string& file, const nebula::meta::CsvProps& csv, const std::vector<std::string>& columns)
    : nebula::surface::RowCursor(0),
      fstream_{ file },
      row_{ csv.delimiter.at(0) },
      cacheRow_{ csv.delimiter.at(0) },
      numCols_{ 0 } {
    // if the file is compressed, we decompress it first
    if (csv.compression == "gz") {
      LOG(INFO) << "Ungzip the csv file before reading: " << file;
      auto localFs = nebula::storage::makeFS("local");
      this->temp_ = localFs->temp();
      nebula::common::ungzip(file, this->temp_);
      this->fstream_ = std::ifstream(this->temp_);
    }

    // a few scenarios need to be handled
    // 1. schema provided
    // 1.a: csv has header - let's match column index to column name by reading header.
    // 1.b: csv has no header - let's assuming the schema is sequential columns of the csv file
    // 2. schema not provided:
    // 2.a: csv has header - we need to read headers to use them as the schema.
    // 2.b: csv has no header - fail, don't know how to process schema
    LOG(INFO) << "Reading csv file: " << file << ", delimiter: " << csv.delimiter;
    std::vector<std::string> names;
    const auto hasSchema = columns.size() > 0;

    // some unicode file has BOM in it that we should skip
    this->skipBOM();

    // scenario 1.b: if the schema is given, has no header
    if (!csv.hasHeader) {
      // 2.b - don't know how to handle
      if (!hasSchema) {
        throw NException("Can't figure out schema without header");
      }

      // schema names provided
      names = columns;
    } else {
      // read the header
      N_ENSURE(row_.readNext(fstream_, 1), "Failed to read csv header unexpectedly.");

      // extract all names
      const auto& raw = row_.rawData();
      for (size_t i = 0, size = raw.size(); i < size; ++i) {
        names.emplace_back(nebula::common::normalize(raw.at(i)));
      }

      // dedup column names
      dedup(names);
    }

    // build the name to index map
    for (size_t i = 0, size = names.size(); i < size; ++i) {
      const auto name = names.at(i);
      // notes: columns could be partial of all data and it should be already deduped
      // example:
      // 1.a csv data has 10 columns, but only 5 columns are provided in the schema
      if (!hasSchema || std::find(columns.begin(), columns.end(), name) != columns.end()) {
        columns_[name] = i;

        // update the column size
        numCols_ = i + 1;
      }
    }

    cacheRow_.setSchema([this](const std::string& name) -> size_t {
      return columns_.at(name);
    });

    // if data has header and header was not consumed yet (to build schema), we have to skip the first row
    DevNull devnull;
    // if data has meta in the second row, skip it too
    if (csv.hasMeta) {
      fstream_ >> devnull;
    }

    // read one row
    if (row_.readNext(fstream_, numCols_)) {
      size_ = 1;
    } else {
      LOG(WARNING) << "Current CSV reader will be empty due to invalid first row: " << file;
    }

    // just log to confirm a reader is created successfully
    LOG(INFO) << "Successfully created csv reader for file: " << file << ", columns: " << numCols_;
  }

  virtual ~CsvReader() {
    if (this->temp_.size() > 0) {
      unlink(this->temp_.c_str());
    }
  }

  // next row data of CsvRow
  virtual const nebula::surface::RowData& next() override {
    // consume a row and read a new row
    cacheRow_.setData(std::move(row_.rawData()));

    // read next row
    // we should handle those to skip less rows
    if (row_.readNext(fstream_, numCols_)) {
      size_ += 1;
    } else {
      // no more data or invalid row meets, we don't skip bad rows
      if (fstream_.eof()) {
        LOG(INFO) << "Finish reading CSV file with total rows: " << size_;
      } else {
        LOG(WARNING) << "CSV reader stops at bad row number: " << size_;
      }
    }

    index_++;
    return cacheRow_;
  }

  virtual std::unique_ptr<nebula::surface::RowData> item(size_t) const override {
    throw NException("CSV Reader does not support random access by row number");
  }

private:
  // ref: https://help.salesforce.com/s/articleView?id=000383918&type=1
  void skipBOM() {
    static const std::string BOM = "\xEF\xBB\xBF";
    char c;
    for (size_t i = 0, size = BOM.size(); i < size; ++i) {
      this->fstream_.get(c);
      if (c != BOM[i]) {
        this->fstream_.seekg(0, std::ios_base::beg);
        return;
      }
    }

    // skip the BOM
    this->fstream_.seekg(3, std::ios_base::beg);
  }

private:
  std::string temp_;
  std::ifstream fstream_;
  CsvRow row_;
  CsvRow cacheRow_;

  // numCols is unnecessary to be the same as size of columns_
  size_t numCols_;
  nebula::common::unordered_map<std::string, size_t> columns_;
};

} // namespace storage
} // namespace nebula